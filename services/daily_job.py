"""
æ¯æ—¥ä»»åŠ¡æ¨¡å— - è‡ªåŠ¨çˆ¬å–æ–°é—»å¹¶ç”Ÿæˆæ—¥æŠ¥æ€»ç»“
"""
from __future__ import annotations

import os
import json
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from typing import List, Dict, Any, Optional, Literal

from .model_service import get_model_service


class DailyJobService:
    """
    æ¯æ—¥ä»»åŠ¡æœåŠ¡
    - çˆ¬å–æ˜¨å¤©çš„æ–°é—»
    - è°ƒç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆæ—¥æŠ¥æ€»ç»“ï¼ˆå­¦ç”Ÿç‰ˆ/æ•™å¸ˆç‰ˆï¼‰
    """
    
    def __init__(
        self,
        news_save_dir: str = "/root/NLP/grab_news/news_days",
        daily_report_dir: str = "/root/NLP/daily_reports",
        base_url: str = "https://dean.xjtu.edu.cn/"
    ):
        self.news_save_dir = news_save_dir
        self.daily_report_dir = daily_report_dir
        self.base_url = base_url
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.news_save_dir, exist_ok=True)
        os.makedirs(self.daily_report_dir, exist_ok=True)
    
    def crawl_news_by_date(self, target_date: str = None) -> List[Dict[str, Any]]:
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸçš„æ–°é—»
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ”¯æŒæ ¼å¼ï¼š
                - YYYY-MM-DD (å¦‚ 2025-11-27)
                - MM-DD (å¦‚ 11-27ï¼Œé»˜è®¤å½“å‰å¹´ä»½)
                - None (é»˜è®¤æ˜¨å¤©)
        
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å« id, url, title, content_clean, publish_time ç­‰å­—æ®µ
        """
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date is None:
            target_mm_dd = (datetime.now() - timedelta(days=1)).strftime('%m-%d')
            target_full = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        elif '-' in target_date and len(target_date) == 10:
            # YYYY-MM-DD æ ¼å¼
            target_full = target_date
            target_mm_dd = target_date[5:]  # æå– MM-DD
        elif '-' in target_date and len(target_date) <= 5:
            # MM-DD æ ¼å¼
            target_mm_dd = target_date.zfill(5)  # ç¡®ä¿æ˜¯ MM-DD æ ¼å¼
            current_year = datetime.now().year
            target_full = f"{current_year}-{target_mm_dd}"
        else:
            print(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {target_date}ï¼Œä½¿ç”¨æ˜¨å¤©")
            target_mm_dd = (datetime.now() - timedelta(days=1)).strftime('%m-%d')
            target_full = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"æ­£åœ¨çˆ¬å–æ–°é—»: {self.base_url}")
        print(f"ç›®æ ‡æ—¥æœŸ: {target_mm_dd} ({target_full})")
        
        try:
            response = requests.get(self.base_url, timeout=30)
            response.encoding = response.apparent_encoding
        except requests.RequestException as e:
            print(f"âœ— è¯·æ±‚å¤±è´¥: {e}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å®šä½æ–°é—»åˆ—è¡¨
        tz_div = soup.find('div', class_='tz')
        if not tz_div:
            print("æœªæ‰¾åˆ° <div class='tz'> å…ƒç´ ")
            return []
        
        li_tags = tz_div.find_all('li')
        print(f"æ‰¾åˆ° {len(li_tags)} ä¸ªæ–°é—»æ¡ç›®")
        
        # æå–ç›®æ ‡æ—¥æœŸçš„æ–°é—»é“¾æ¥
        results = []
        for li in li_tags:
            link = li.find('a')
            span_tag = li.find('span')
            
            if not link or not span_tag:
                continue
            
            href = link.get('href', '')
            title = link.get('title', '')
            span_text = span_tag.get_text().strip()
            
            # åªå¤„ç†ç›®æ ‡æ—¥æœŸçš„æ–°é—»
            if span_text == target_mm_dd:
                results.append({
                    'href': href,
                    'title': title,
                    'span': span_text,
                    'full_date': target_full
                })
                print(f"  âœ“ å‘ç°æ–°é—»: {title}")
        
        if not results:
            print(f"ç›®æ ‡æ—¥æœŸ ({target_mm_dd}) æ²¡æœ‰æ–°é—»")
            return []
        
        # è®¿é—®æ¯æ¡æ–°é—»è¯¦æƒ…é¡µ
        crawl_time = datetime.now().strftime('%Y-%m-%d')
        news_data = []
        
        for i, item in enumerate(results, 1):
            if not item['href']:
                continue
            
            # æ„å»ºå®Œæ•´URL
            if item['href'].startswith('http'):
                full_url = item['href']
            else:
                full_url = urljoin(self.base_url, item['href'])
            
            print(f"[{i}/{len(results)}] æ­£åœ¨çˆ¬å–: {item['title'][:30]}...")
            
            try:
                news_response = requests.get(full_url, timeout=10)
                news_response.encoding = news_response.apparent_encoding
                news_soup = BeautifulSoup(news_response.text, 'html.parser')
                
                # æå–æ ‡é¢˜å’Œå‘å¸ƒæ—¥æœŸ
                article_title = item['title']
                publish_date = None
                
                art_tit_div = news_soup.find('div', class_='art-tit cont-tit')
                if art_tit_div:
                    h3_tag = art_tit_div.find('h3')
                    if h3_tag:
                        article_title = h3_tag.get_text().strip()
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    for span in art_tit_div.find_all('span'):
                        span_text = span.get_text().strip()
                        if 'å‘å¸ƒæ—¥æœŸ' in span_text or 'å‘å¸ƒæ—¶é—´' in span_text:
                            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', span_text)
                            if date_match:
                                publish_date = date_match.group(1)
                            break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‘å¸ƒæ—¥æœŸï¼Œä½¿ç”¨ç›®æ ‡æ—¥æœŸ
                if not publish_date:
                    publish_date = item.get('full_date', '')
                
                # æå–æ­£æ–‡å†…å®¹
                content_text = ""
                content_div = news_soup.find('div', class_='v_news_content')
                if content_div:
                    content_text = self._clean_content(content_div)
                
                news_item = {
                    "id": f"news_{crawl_time}_{i}",
                    "url": full_url,
                    "source": "æ•™åŠ¡å¤„",
                    "publish_time": publish_date or "",
                    "crawl_time": crawl_time,
                    "title": article_title,
                    "content_raw": "",
                    "content_clean": content_text,
                    "attachments": []
                }
                
                news_data.append(news_item)
                print(f"  âœ“ æˆåŠŸçˆ¬å–")
                
            except Exception as e:
                print(f"  âœ— çˆ¬å–å¤±è´¥: {e}")
        
        # ä¿å­˜æ–°é—»æ•°æ®
        if news_data:
            filename = f"news_{target_mm_dd.replace('-', '')}.json"
            save_path = os.path.join(self.news_save_dir, filename)
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ å·²ä¿å­˜ {len(news_data)} æ¡æ–°é—»åˆ°: {save_path}")
        
        return news_data
    
    def crawl_yesterday_news(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ˜¨å¤©çš„æ–°é—»ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        return self.crawl_news_by_date(target_date=None)
    
    def load_news_from_file(self, target_date: str) -> Optional[List[Dict[str, Any]]]:
        """
        ä»å·²ä¿å­˜çš„æ–‡ä»¶åŠ è½½æ–°é—»æ•°æ®
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD æˆ– MM-DD)
        
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å› None
        """
        # è§£ææ—¥æœŸè·å– MM-DD æ ¼å¼
        if len(target_date) == 10:
            mm_dd = target_date[5:]
        else:
            mm_dd = target_date.zfill(5)
        
        filename = f"news_{mm_dd.replace('-', '')}.json"
        file_path = os.path.join(self.news_save_dir, filename)
        
        if os.path.exists(file_path):
            print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ–°é—»: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return None
    
    def _clean_content(self, content_div) -> str:
        """æ¸…æ´—HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
        # å¤„ç†æ®µè½æ ‡ç­¾
        for p_tag in content_div.find_all(['p', 'div']):
            if p_tag.get_text(strip=True):
                p_tag.append('\n')
        
        content_text = content_div.get_text(separator=' ', strip=True)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content_text = re.sub(r' +', ' ', content_text)
        content_text = re.sub(r' \n', '\n', content_text)
        content_text = re.sub(r'\n ', '\n', content_text)
        content_text = re.sub(r'\n{2,}', '\n', content_text)
        content_text = content_text.strip()
        
        # æ¸…ç†æ¯è¡Œ
        lines = content_text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line:
                line = re.sub(r'\s+', ' ', line)
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def generate_daily_report(
        self,
        news_list: List[Dict[str, Any]],
        date_str: Optional[str] = None
    ) -> Dict[str, str]:
        """
        ç”Ÿæˆæ¯æ—¥æ–°é—»æ€»ç»“æŠ¥å‘Šï¼ˆå­¦ç”Ÿç‰ˆ + æ•™å¸ˆç‰ˆï¼‰
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        
        Returns:
            åŒ…å« student_summary å’Œ teacher_summary çš„å­—å…¸
        """
        if not news_list:
            return {
                "date": date_str or datetime.now().strftime('%Y-%m-%d'),
                "student_summary": "ä»Šæ—¥æ— é‡è¦æ–°é—»é€šçŸ¥ã€‚",
                "teacher_summary": "ä»Šæ—¥æ— é‡è¦æ–°é—»é€šçŸ¥ã€‚"
            }
        
        if not date_str:
            date_str = news_list[0].get('publish_time', datetime.now().strftime('%Y-%m-%d'))
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        blocks = [f"ã€æ—¥æœŸã€‘{date_str}"]
        for idx, news in enumerate(news_list, 1):
            blocks.append(
                f"ã€æ–°é—»{idx}ã€‘\n"
                f"æ ‡é¢˜ï¼š{news.get('title', '')}\n"
                f"æ¥æºï¼š{news.get('source', 'æ•™åŠ¡å¤„')}\n"
                f"å‘å¸ƒæ—¶é—´ï¼š{news.get('publish_time', '')}\n"
                f"æ­£æ–‡ï¼š\n{news.get('content_clean', '')[:2000]}\n"  # é™åˆ¶é•¿åº¦
            )
        
        news_content = "\n".join(blocks)
        
        # è·å–æ¨¡å‹æœåŠ¡
        model_service = get_model_service()
        
        print("\næ­£åœ¨ç”Ÿæˆå­¦ç”Ÿç‰ˆæ—¥æŠ¥...")
        student_summary = model_service.summarize_news(news_content, user_identity="student")
        
        print("æ­£åœ¨ç”Ÿæˆæ•™å¸ˆç‰ˆæ—¥æŠ¥...")
        teacher_summary = model_service.summarize_news(news_content, user_identity="teacher")
        
        report = {
            "date": date_str,
            "news_count": len(news_list),
            "student_summary": student_summary,
            "teacher_summary": teacher_summary,
            "generated_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ä¿å­˜æ—¥æŠ¥
        report_filename = f"report_{date_str}.json"
        report_path = os.path.join(self.daily_report_dir, report_filename)
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ æ—¥æŠ¥å·²ä¿å­˜åˆ°: {report_path}")
        
        return report
    
    def run_daily_job(self, target_date: str = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ¯æ—¥ä»»åŠ¡æµç¨‹
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD æˆ– MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
        
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœï¼ŒåŒ…å«çˆ¬å–çš„æ–°é—»æ•°é‡å’Œç”Ÿæˆçš„æ—¥æŠ¥
        """
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date is None:
            target_full = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            date_display = "æ˜¨å¤©"
        elif '-' in target_date and len(target_date) == 10:
            target_full = target_date
            date_display = target_date
        elif '-' in target_date:
            mm_dd = target_date.zfill(5)
            current_year = datetime.now().year
            target_full = f"{current_year}-{mm_dd}"
            date_display = target_full
        else:
            target_full = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            date_display = "æ˜¨å¤©"
        
        print("=" * 60)
        print(f"ã€æ¯æ—¥ä»»åŠ¡å¯åŠ¨ã€‘ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ã€ç›®æ ‡æ—¥æœŸã€‘ {date_display} ({target_full})")
        print("=" * 60)
        
        # Step 1: å°è¯•ä»æ–‡ä»¶åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™çˆ¬å–
        print(f"\nğŸ“° Step 1: è·å– {target_full} çš„æ–°é—»")
        news_list = self.load_news_from_file(target_full)
        
        if news_list is None:
            print("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–...")
            news_list = self.crawl_news_by_date(target_date=target_full)
        else:
            print(f"âœ“ ä»æ–‡ä»¶åŠ è½½äº† {len(news_list)} æ¡æ–°é—»")
        
        if not news_list:
            print(f"\nâš ï¸ {target_full} æ²¡æœ‰æ–°é—»ï¼Œç”Ÿæˆç©ºæ—¥æŠ¥")
            # ç”Ÿæˆç©ºæ—¥æŠ¥
            empty_report = {
                "date": target_full,
                "news_count": 0,
                "student_summary": "ä»Šæ—¥æ— é‡è¦æ–°é—»é€šçŸ¥ã€‚",
                "teacher_summary": "ä»Šæ—¥æ— é‡è¦æ–°é—»é€šçŸ¥ã€‚",
                "generated_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            # ä¿å­˜ç©ºæ—¥æŠ¥
            report_filename = f"report_{target_full}.json"
            report_path = os.path.join(self.daily_report_dir, report_filename)
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(empty_report, f, ensure_ascii=False, indent=2)
            print(f"âœ“ ç©ºæ—¥æŠ¥å·²ä¿å­˜åˆ°: {report_path}")
            
            return {
                "status": "no_news",
                "news_count": 0,
                "report": empty_report
            }
        
        # Step 2: ç”Ÿæˆæ—¥æŠ¥
        print("\nğŸ“ Step 2: ç”Ÿæˆæ—¥æŠ¥æ€»ç»“")
        report = self.generate_daily_report(news_list, date_str=target_full)
        
        print("\n" + "=" * 60)
        print("ã€æ¯æ—¥ä»»åŠ¡å®Œæˆã€‘")
        print(f"  - æ–°é—»æ•°é‡: {len(news_list)}")
        print(f"  - æ—¥æŠ¥æ—¥æœŸ: {report['date']}")
        print("=" * 60)
        
        return {
            "status": "success",
            "news_count": len(news_list),
            "report": report
        }
    
    def get_report_by_date(self, date_str: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„æ—¥æŠ¥
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        
        Returns:
            æ—¥æŠ¥å†…å®¹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        report_path = os.path.join(self.daily_report_dir, f"report_{date_str}.json")
        if os.path.exists(report_path):
            with open(report_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def get_recent_reports(self, days: int = 7) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘Nå¤©çš„æ—¥æŠ¥
        
        Args:
            days: å¤©æ•°
        
        Returns:
            æ—¥æŠ¥åˆ—è¡¨ï¼ˆæŒ‰æ—¥æœŸå€’åºï¼‰
        """
        reports = []
        today = datetime.now().date()
        
        for i in range(days):
            date = today - timedelta(days=i)
            date_str = date.isoformat()
            report = self.get_report_by_date(date_str)
            if report:
                reports.append(report)
        
        return reports


# ä¾¿æ·å‡½æ•°
def run_daily_job() -> Dict[str, Any]:
    """æ‰§è¡Œæ¯æ—¥ä»»åŠ¡"""
    service = DailyJobService()
    return service.run_daily_job()


if __name__ == "__main__":
    run_daily_job()

